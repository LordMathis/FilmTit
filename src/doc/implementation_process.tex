\section{Timeline}

The following section describes the project development, progress and decision as we did through the time the the project was developed. After series of informal talk the project officially started in December 2011 and was submitted in September 2012.

\subsection{Initial project meetings and early implementation decisions}

The regular project meeting where we discussed mostly the organizational aspects and the top-level design of the application started in December 2011. We easily agreed that we were aiming to create an application quite similar to the Google Documents. We have decided very early to use the multi tier architecture consisting of

\begin{itemize}
\item core translation memory,
\item user space,
\item graphical user interface,
\end{itemize}

which became very soon separate Maven modules (as soon started using Maven for building the project). Despite we added further modules later, we consistently kept the initial project splitting into \emph{Core}, \emph{User Space} and \emph{GUI}. At the beginning we also assigned team members to the different part of project which remained surprisingly stable as well. 

Before receiving the data from OpenSubtitles.org we were thinking about the source of data to fill the translation memory for the first time. There were several options -- either using the subtitle part of the Czech-English parallel corpus CzEng developed at ÃšFAL, using sentences from a general purpose parallel corpus or getting the data from a subtitle server.

From the very beginning we intended base the project on Java. Mainly because there exist quite a lot of web technologies based on Java and everybody of us were at least a little bit familiar with it. We also decided to combine the code in Java and Scala programming languages and Apache Maven as a build manager. At that time there was only one team member who knew the Scala language. Despite we repetitively expressed believes that everybody of would learn it, at the end there was one another person familiar with the Scala language which appeared to be a bottleneck of work on the translation memory core.

Much complicated was to agree on the technology of the client. There were many different opinions from writing the client in {\it PHP} with {\it Nette Framework} which some of us know quite well, using the {\it JSP} to have all the code consistently in Java or even quite extreme idea to make the whole application as {\it Java Applet} (this idea had appeared because of the intention to integrate the video player in the application and at that time we didn't of any other option of doing it except making a Java applet for it). Finally we decided to use {\it Google Web Toolkit} which nobody of us had had any prior knowledge before, but it promised making the communication between server and client very easy. Similarly to the Scala language we ended up with just two people able to work effectively with the Google Web Toolkit. Fortunately, it did not became a bottleneck of the development process. 

After overcoming the problems connected with learning a new technology we became quite satisfied with the decisions because the usage of Google Web Toolkit and other modules, as well as the possibility to combine all parts of the project with Apache Maven were helpful despite making the project run in Maven was very painful for us and we spent unnecessarily a lot of time by solving this issues.
	
The decision to use both Java and Scala appeared to be a bring quite a lot of complications. On the one hand, Scala allowed to write concise and efficient code, on the other hand most project members were not able to learn Scala sufficiently and hence used only Java. Although the interoperability between Scala and Java works well in most cases because both are based on the JVM, some problems remain. One of the problems of interoperability was, for example, that the implementation of the data type \emph{List} that was created in Scala was not compatible with Google Web Toolkit, which expected a standard Java \emph{List} implementation.

\subsection{Early development process}

Luckily for us we received very soon a database from the {\it opensubtitles.org} containing all the Czech and English subtitle file that were at the server at that time. Soon after that we started an alignment algorithm to retrieve the parallel data from the subtitle files (the process is described in section~\ref{sec:aligning_subtitles}) and enable us to start experiments that helped us to decide which database system could be used.

Choosing appropriate database system was also intensively discussed issue. The database underlying the Translation Memory plays a crucial role for the whole system performance. Also using built-in features could save us a lot of additional work.  We evaluated different DBMS and decided to use \postgres~(see section~\ref{sec:dbms} for details). To test its applicability for our project, we ran several small evaluations of \postgres~features that might be useful to us, e.g.\ the full-text search. As soon as we made the final decision on which DBMS to use, we re factored the evaluation code into {\tt TranslationPairSearcher} classes and started to set out the general architecture of the core. The {\tt BackoffTranslationMemory} class was added and the candidate search was finished in the early stages, so a basic version of the system could be used even though there were not yet any sophisticated rankers. For more details on the core architecture, please see Section~\ref{sec:corearchitecture}.

Not having any experience with the object-relation mapping libraries in Java we started to us {\it Hibernate} based on the Internet forums and advice of our colleagues.

Originally, all the algorithms precessing the data we retrieved were implemented in Perl the and the code importing the data were in the \emph{Core} module implemented in Scala. Later, to be the code more consistent, we decided to move the data preparation and data import to a separate \emph{dataimport} module and re-implement the Perl scripts in Scala.

\begin{figure}[h]
\begin{center}
\includegraphics{./figures/original_strucutre.pdf}
\end{center}

\caption{Scheme of the originally intended structure of work with the translation memory. It reflects the original User Space structure and also schematically the original client design.}\label{fig:original_scheme}

\end{figure}

We started to solve very soon the video playback in the browser which we expected to be a really challenging issue. We did some research about \emph{Adobe Flash} technology. We were also thinking about creating a hybrid solution -- an application wrapper with web browser inside, capable to ensure the video playback for the inner web application.

We very soon had toy implementations of:

\begin{itemize}
\item a player using VLC plugin, to be incorporated into the web app

\item a player as a desktop java app, showing the web app in a frame
\end{itemize}

\begin{figure}[h!]
	\centering
		\includegraphics[width=7cm]{figures/desktop-app-player.png}
	\caption{Screenshot of Qt-based desktop application.}
	\label{fig:figures_desktop-app-player}
\end{figure}

We decided for the web app only version, with no desktop variant, because, observing the development trends, it seems to us that the present and future of applications is in web applications. The benefits are e.g. that the learning curve is typically better for a web application, where the user is already familiar with most of the controls and work patterns, installation is not required so anyone can start using the application immediately, it is easy to use OpenID registration... Disadvantages: cross browser support, limited power, larger bandwith consumption. GWT added advanatage: the bandwith consumption is closer to that of a standalone desktop app, because the application itself is stored in one Javascript file, which is downloaded at the beginning (similar to installing a desktop application), and the rest of the client-server interaction is done through RPCs (basically the same way as it would be done in a desktop application).


TODO: player -- Karel should write something more about its development, the applet signing, etc.

An issue we encountered in developing the player and were not able to combat completely is letting the user choose a media file and passing its full filesystem path to the player. We found out that although it is quite easy to load a whole file from disc into memory, only its filename is passed to Javascript instead of the full path for security reasons. We did a thorough search on the internet but found out that there is most probably no clean way around this restriction.

Ultimately we came up with 3 solutions, but none of them pleases us enough:

- the user inputs the full path as a string into an input box; works well but is not user-friendly (most users probably do not even know that such a thing as a filepath exists)

- the user chooses the file in the file browser and then copies the file path from the browser inputbox into a textbox; maybe even less userfriendly

- a Java applet is loaded to choose the file as Java applets do not have such restrictions, and then passes the path to the application; it seems to us as an \todo{overkill} to create and load a whole applet only to get the file path, but it works quite well -- except for the users who do not have Java installed and working properly

We decided to use the third option, with the first option also available as a fallback for users who do not have Java running properly and thus cannot use the applet.

\subsection{Introducing the shared classes}
\label{subsec:introducing_shared_classes}

After having implemented very basic version of both three parts of the project we decided it was time to start to solve the interoperability of individual parts in order to run a first snapshot of the application. This was happening approximately in March 2012.

At this stage we found out that we are not fully taking advantage of using the Java technologies for all parts of project and decided to totally redesign the User Space and Client parts.

Originally we wanted to keep the traditional translation memory structure where each sentence can have several matches and these matches can have several translations in the memory as is depicted in the figure \ref{fig:original_scheme}. Anyway, this scheme did not reflect much the way we worked with the data in the that time -- suitable matches were provided from the table of translation pairs based on indexes built over the table. Moreover, the were very few matches having more different translations in the data and the design of the GUI of the application was a little bit confusing because it used to place the text of matches to prominent place despite the user is not as concerned about the matches as he is about the actual translation suggestions. This lead to a scheme which is depicted in figure \ref{fig:new_scheme}.

\begin{figure}
\begin{center}
\includegraphics{./figures/current_strucutre.pdf}
\end{center}
\caption{Current scheme of the work with translation memory.}\label{fig:new_scheme}
\end{figure}

We agreed on the shared classes that all parts of the project should use.  We more or less adopted the design of the classes from the core and started to use in the whole project. This step required to re-implement the classes from Scala to Java and to drop a lot of code that was already done in the User Space and the client. The design of the classes was almost the same as is described in \ref{sec:shared_structure}.

We started the work on the project with new design soon, anyway it started the period of the biggest struggle with technologies. It took almost two months to have a first running version of the application.

The very first version of the application was a page where it was possible to upload a subtitle file do the translation without any possibility load an already saved subtitle document or download a result of the translation. without any sessions, user, it was just a page, where you edit the subtitles. At that time we used machine translation from the MyMemory service which is in fact Google Translate, anyway there is a limited acces per IP address, so it became obvious the we would have to change the source of machine translation. We used API of IMDB.com, to receive information about movies but the movie meta data was not used in the evaluation the matches at that time.

From the later view it appeared to be an important decision to agree on the shared classes which made the cooperation between the modules easier and less verbose. 

\subsection{The Main Development Phase}

It is hard to define the main development phase which is covered in the following paragraphs. It corresponds to the period from the beginning of March when the important design changes were made (see previous section \ref{subsec:introducing_shared_classes}) to approximately the middle of August when adding new features was stopped (see next section \ref{subsec:final_development}).

The section is divided to parts covering the issues we were dealing with and describes what we did in each area in those approximately five and half months.

\subsubsection{OpenID}
\label{subsubsec:openid}

Despite we wanted to implement OpenID support from the very beginning we started with it approximately in May. We found two most frequently used Java libraries for OpenID and tried them. Those were \emph{JOpenID} and \emph{openid4java}.

\emph{JOpenID} seemed to be easier to use and also was much smaller as a dependency, on the other hand when we run into some problems with using it we found out that \emph{openid4java} is much more frequently discussed on the \emph{StackOverflow.com} forum and that there bigger chance that we would be able to find some advice for potential problems.

\emph{Openid4java} library is much bigger than then \emph{JOpenID} (it has megabytes whereas \emph{JOpenID} has tens of kilobytes\todo{check if it is true}) and supports not only logging in but also providing the OpenID endpoint service. When found out that the problems with the libraries was because of wrongly set Maven dependencies we turned back to \emph{JOpenID} library which is much easier to use.

At the end we decided to use \emph{JOpenID} library and support Google, Yahoo and Seznam accounts. The library contains the Google and Yahoo by default. We decided to also add support for \emph{Seznam.cz} as the biggest Czech OpenID provider. Seznam provides the authentication data (we are particularly interested in the user name and email which we use in our application) in a different form than the other OpenID providers, therefore it was necessary to write our parser of the authentication data ({\tt{cz.filmtit.userspace.login.SeznamData}}).

We do not support generic OpenID because the lib does not support it. Anyway, after doing some research on the Czech developers forums we concluded that just a very small percentage of users actually know what OpenID is and allowing to login via the Google and Seznam accounts is hopefully enough.

The support for OpenID was finished in July, the parser for the authentication data from Seznam were added at the beginning of August.

A question to solve was what to do when the user logs in through OpenID for the first time. We were thinking about requiring an explicit registration from the user in such case, but then decided that we probably do not need any more data than we already get from the OpenID provider. Therefore, we decided to register the user transparently in such situation.
However, because it can happen that e.g. the OpenID provider is temporarily unavailable, ceases to provide his services or that the user closes his account at the provider, we decided to offer the user a possibility to log in to his account even without the OpenID. For that reason, we create a full account for such user, generating a username and password and sending these to his e-mail address (which usually can be acquired from the OpenID provider).

Because the generated username is not only used as a backup way of authentication but is also displayed to the user in the top right corner of the application, we wanted to generate one that the user would recognize as being his. At first, we simply took the username part of his e-mail address (i.e. the part before the @ sign) and used it directly. However, this eventually leads to collisions, so we had to implement something more sophiticated.

The most primitive solution was iteratively trying to append numbers to the username if it was already taken, starting with 1, and checking whether such username was available. This was then improved to starting not at 1 but at the number of occurences of a username with the wanted username as a prefix, thus heuristically skipping the other usernames probably created in a similar way (and probably a few more just to be sure). And finally, we changed the iterative step from a simple increment to multiplication by 2, breaking any possibly remaining long sequences of already existing usernames.

Moreover, the user has the possibility to change his username and/or password on the Settings page.

\subsubsection{Chunk Splitting}

TODO

ideas:

keep as it is, i.e. one timing = one chunk ... too long, too arbitrary

split into sentences, joining already split sentences together ... very hard (probably impossible) to do reliably, would miss the advantage of subtitle files for alignment

now: keep already existing splits, split on dialogue boundaries, slit on sentence boundaries (+ clean away irrelevant characters)

... easy enough to do reliably (SOME NUMBER of errors)

... arbitrary but not so much as the first solution

... length is reasonable

Important: to do the splitting in the same way when creating the database and when processing the user subtitles.
Therefore: a shared Parser class which is compiled both into java bytecode and used in impoting, and into Javascript and used in GUI

\subsubsection{Chunk Time Format}

TODO

we used strings

then we needed to parse these for player and for editing the timing

decided to use the SRT format only (99\% of subtitles we have and easier to use)

a class (SrtTime) that represents the time as a set of integers, providing constructors, getters and setters for all formats we need -- one string, a set of strings, a set of integers, one integer (the time in miliseconds), + compareTo, + subtract

\subsubsection{Chunk Annotations}

TODO

named entities = first reason for annotations

chunk splitting by dashes denoting individual dialogue speekers: dashes need to be removed for translation suggestions generating because they are irrelevat but need be kept for correct rendering of text -> annotations extended to contain dialogue dashes

need to identify the newlines (irrelevant for translation suggestions but necessary to render the subtitles properly) -> annotations extended for newlines

subtitle formatting for bold, italic, underline etc. Original idea: extended the annotations again (easy to do). Then: too much trouble, expecially for GUI. Decision: just strip off these, do not support that. Reasoning: used very little, no formal specification so has various formats, often the players do not support the formatting properly so we want to discourage the users from using the formatting

chunk now has a set of methods to get and set the chunk and annotations correctly, using the notion of FORMS. The forms used are:

database form: the text as stored in the subtitle file, only cleaned -- formatting stripped off (bold, italic etc.), non-printable characters turned into spaces, multiple spaces replaced by one space only... The newlines are stored as PIPE characters.

surface form + annotations: the surface form is the text to be used to generate the translation suggestions, without dashes and with newline turned into spaces

GUI form: HTML form to be displayed in GUI, i.e. newlines turned into BR tags

text form: to be used in textarea -- with dashes, newlines as BACKSHLASH N characters

\subsubsection{Logging}

In US: easy, we use (whatever we use)


In GUI: tricky -- we do not have any "console" so we cannot simply write logs to "standard output"
(actually we have a console, but only in dev mode)

preliminary solutions: "debug prints" ensured by alerts, changing the page title, changing the page statusbar -- not usable in larger scale

development solution: a debug window at the bottom of the page, and a (in the end) static method Gui.log(String) that prints the log messages into that window
... proved to be very useful, with a large enough amount of logging we were often able to locate the bugs from the contents of this window

also added an exception catcher which gets the stacktrace from the exception, then extended to handle umbrella exceptions as well (= multiple exceptions in one)

great: GWT provides stacktraces with filenames and line numbers from the source java code
downpoint: not very reliable but usually useful

great: GWT provides a possibility to catch all exceptions -- using GWT.setUncaughtExceptionHandler() -- it has no real "main" so one cannot just put that into a try-catch block

issue with debug window: cannot appear in final application :-) but still there might be errors;
also it is at the user side, but we need the logs at the server side;
ultimate solution: "remote logging", i.e. important messages are sent to the server through an RPC, where they are both included into the server log and saved into DB for possible future use (together with user id)
The minimal level of a meesage to be sent to the server can be set (debug, info, warn, error)

\subsubsection{Implementing the RPCs}

From the start it was obvious to us that we want to use a well-estabilished method to implement the Remode Procedure Calls. Our first idea was to directly send and receive JSON messages because they have a format of a Javascript object, so we thought that this would be easy and efficient to implement in GWT (it is possible to read the object from string into memore simply using Javascript eval() function). However, there are three facts that we later found out that made us change our decision. The first one is that, although it is possible to directly use eval() function to convert the JSON message into the in-memory object representation, this practice is not recommended for security reasons (any Javascript code can be injected and it would be directly executed with no constraints). Therefore, the recommended practice is to use a JSON parser, which validates the JSON object before calling eval(). A related issue is that although GWT sometimes does use JSON internally, it does not provide a JSON parser to be used by the programmer, so one has to use an additional library. And finally, we found out that GWT already implements a mechanism to send RPCs (in which, as a matter of fact, the messages are actually serialized to JSON).

The type of an RPC parameter or return value can be any GWT-compilable type. Because this type must be known both to the server and to the client, we use only primitive types and shared classes SEE XXX. Each class sendable through GWT RPC mechanism must implement the GWT IsSerializable interface (which is only a marker, i.e. it does not have any abstract methods) and a parameter-less constructor; this also holds for the checked exceptions, which are technically only a special type of a return value.

Because of Javascript and browser limitations, the RPC mechanism of GWT only allows asynchronous calls from client to server. This suited our needs often, but we had to go around these limits in some situations. Quite often we need the RPC to be synchronous and blocking, e.g. in logging in or changing user settings. In such situations, the page or dialog is "deactivated" on invoking the RPC (all active components are disabled -- e.g. the textboxes are read-only, the buttons do not respond to clicks), a reference to the page or dialog is passed to the class invoking the RPC, and when the RPC returns, the page or dialog is "reactivated", either with a success message or an error message, depending on the result of the RPC.

In one case, we would need an opposite direction RPC, i.e. the server to invoke the RPC on the client. This is when authenticating the user through OpenID, where we have to perform the authentiation in a new window -- the new window then receives the result of the authentication, but it cannot be easily passed directly into the opener window. Here we decided to use active polling -- the opener window regularly invokes the GetSessionID RPC, which returns null if the authentication process has not yet finished, this being a signal for the GUI to continue polling. To link together the opener window and the authentication window, a unique authID is used. Originally it was created by the GUI and sent to Userspace, but we soon found this to be inappropriate (TODO why -- collisions, anything else?) and the Userspace now generates the authID.

Most of the RPCs contain the SessionID as one of their parameters to authenticate the user. All of such RPCs can thus throw an InvalidSessionIdException, to which the default GUI action is to show the Login Dialog to the user.

\subsubsection{Failed RPC Requests Handling}

Originally, all the RPC requests followed the barebone AsyncCallback interface, where the request is sent to the server, and onSuccess() or onFailure() method of the callback are called once it returns, depending on the result. The default implementation of onFailure() was to display the error message to the user, the idea behind being that it is eiher a mistake of the user that he should be informed about, or a mistake of the server which he should be informed about as well. This was sufficient for most of the cases; however, it became clear to us that time to time, the requests fail for various reasons beyond our reach (usually temporary network problems), where the default behavior of simply informing the user (and usually throwing away the data he generated for the request) is often not welcome.

We decided that our general error handling strategy should be to try to solve all problems without user interaction, and to inform the user only about user-generated errors (such as incorrect SRT file format or session timeout) or critical errors (such as permanent loss of connection to server).

The first thing was to turn the calls from methods into objects, so that they store their parameters and can be called again on failure. We also created a generic superclass for them, Callable, filled with necessary boilerplate code and methods performing the default actions, to be overridden if necessary. If the request fails for any reason, it is retried by default; four attempts are made for each request, always retrying after a short time interval. We identified the response that we usually get in case of network problems -- which could be temporary or permanent (it is a StatusCodeException with the status code 0). In such case the request is always resent three times. Otherwise, resending the request is the default behavior which the subclasses override in cases where it is obvious that resending will not help (e.g. incorrect e-mail address format or an already existing username on registration). We also added a timeout for each request after which the request is regarded as lost and is retried.

The subclass can always intervene by overriding several methods, such as onSuccessAfterLog, onEachReturn, onFailureAfterLog, onProbablyOffline etc. This way, the behavior needed can be always achieved, but the default implementation is often sufficient, so the subclasses often override only one or two methods, keeping their code as simple as possible. The superclass also provides logging for all the requests, based on the classname and the parameters of the subclass.
(E.g. when deleting a document, a failure with an InvalidDocumentIdException, meaning that the document does not exist, is an error, but there is no need to inform the user because the result is the same as if the RPC succeeded: the document does not exist now.)

\subsubsection{Sending Translation Results}

At first, the translation suggestions from the Translation Memory were requested and sent for each chunk individually. This had the theoretical advantage that the user received each of the translation suggestins as soon as possible. However, we found the overhead of the RPC calls to be unreasonable. Not only was the overall time necessary to load all the suggestions too long, but also a typical browser was unable to handle such a large number of requests and usually froze for long periods of time, often unfreezing only after all of the suggestions arrived.

A simple solution seemed to be sending all of the chunks for translation in one request. However, such a request can take several minutes to complete, and we still wanted to keep the nice property of the application showing the suggestions as soon as possible at least for the first lines -- so that the user can start the translation immediately. Thus, we decided to send the request in batches, starting with a small size of the batch for the beginning of the file and gradually increasing the batch size.

The first way to do that was an exponential increase, where each batch contains twice as many chunks as the previous one, starting with a batch of size one. This proved to solve the aforementioned problem of browser freezing; however, another problem arose. As the batch sizes grew larger and larger, the later requests often took several tens of seconds or even minutes to complete. Meanwhile, the user would often navigate away from the Translation Workspace, rendering the translation suggestions, including the currently being processed ones, obsolete. This was of course detected in GUI and the suggestions were thrown away on arrival, but still it meant that the server was working in vain for some time, possibly resulting in an unnecessarily slower responsiveness to other users (and even to the very same user as well).

We introduced two further solutions. The first and simple one was limiting the size of the batch to 64 chunks (which typically only take about ??? seconds to process), so that the amount of useless work could not grow too large. The second one, which took us some time to think up and implement, was to stop the translation suggestions from being generated on the server, by issuing a StopTranslationResults request. (This breaks the typical model of procesing the queries, where once a query is sent, its processing is not influenced by queries that arrive later.)

The two major issues with the StopTranslationResults request were:

- to interrupt the suggestions generating which is already in process (which is done in parallel for the chunks in the batch, making this even harder).

- to identify what to stop and what to let continue.

The first idea was to mark the whole document as closed, and to check this when generating the suggestions. However, this would cause problems if the same document was immediately reopened by the user, and even larger problems if the user already had the document opened twice -- this can easily happen by mistake, the logical reaction of the user is then to close one of the Translation Workspaces, and then he would stop receiving suggestions even in the other opened Workspace. Thus, we then decided to create an identifier for each Translation Workspace instance and to bind the StopTranslationResults request to this identifier. However, we soon found out that this would require a lot of overhead only to handle this probably not very common situation, so we dropped that idea as well.

Finally, we decided for a compromise solution between those two. The StopTranslationResults request is now bound to individual chunks, which now have an added boolean field isActive. This field is set to true when the GetTranslationResults request arrives to the server, for each of the chunks on the request. The StopTranslationResults request contains a list of the chunks which translations should be stopped. On arrival of that request, Userspace simply sets the isActive field to false -- because Userspace and Core share the same memory and because only references to the chunks are passed around, this change is visible in core even though the chunks have already been sent for translation. The necessary overhead is minimal, and there is no "false alarm" in the typical case. The only unsolved case is the situation where the same batch of chunks are sent for translation at the same time from two different instances of Trabslation Workspace with the same document opened, and then one of these Workspaces is closed, causing the suggestions generation to stop even for the batch from the other Workspace. Although this situation is unlikely and non-critical (the suggestions would be missing only for this one batch, the later batches would be translated correctly), we still offer a general solution by repeating all request on failure (unless it is obvious that the failure will reoccur).

\subsubsection{User Management}

At the very beginning the only the GUI was basically only the translation workspace. It was possible to translate a subtitle file in the application, the changes were reflected in the database, anyway it was not possible neither to get them in the application when the web application was reloaded neither to export the subtitles.

We considered the user management to be just a minor technical issue. Anyway, to enable to reach the intended architecture of the User Space -- mostly that most of the calls should be resolved in the Session objects -- we introduced a simple fake login in April (you could log as any user using password "guest").

We stared to implement the OpenID login support in May (see section \ref{subsubsec:openid}). When we met some technical difficulties. We decided to implement first our own registration and login.

Because our own experience that requiring a registration from a web service often prevent us from using it we decided to make the registration as easy as possible. We have no constraints on user name (except for non-emptiness) and very little constraints on password (only it must be at least characters long).

We think the most important feature of a password is for the user to remember it, we leave the assessment of the strength to the user. The password strength constraints are often rather arbitrary and many classes of strong passwords do not pass them, it is hard to develop a reasonable strength measure. E.g. a whole sentence, a "passphrase", is a very strong password; however, it usually does not pass measures requiring use of specific classes of characters. And vice versa, a password using very unusual characters (e.g. Czech letters with diacritic signs) can be impossible to break even if it is quite short, because the attacker simply does not try out such characters that are uncommon in passwords; still it often does not pass typical minimum password length constraints.

Moreover, we are not forced to pay much attention to the security of the application - it does not contain any sensitive data whose compromising would pose a threat to the users of the application and the benefit to the attacker successfully breaking into a user account is close to none.

We allow the user to work in multiple browser tabs or windows, provided it is the same session (i.e. has the same SessionID), because we believe it is natural to work on multiple translations at once, to have the Document List opened to see the translation progres, etc. The user himself is responsible for having the same document opened multiple times, which is not expected behavior and so changes to the document in one Translation Workspace are not projected into the other Workspace until reload.

However, we decided to disallow one user having multiple sessions at one time, as we believe that this is neither expected nor required behavior, and could potentially confuse the user. We believe that such a state is usually caused by a mistake, e.g. user A forgetting to log out on PC 1, then moving to PC 2, and at the same time user B using the application on PC 1 under user A's account by mistake.
We do not support multiple users collaborating on one subtitle file.

If user registers through OpenID, we have to assign a user name to him (the unique OpenID identifier we get is typically a long hash, which does not look like a nice user name), see section \ref{subsubsec:gui_openid}. Because the user name is at least partly generated, the user might not like it. Therefore, we offer each user the possibility to change the user name (provided that the one he chooses is free). The user name is only used to authenticate the user (if not using OpenID login) and is displayed in the top right corner in the application; the internal unique identifier of a user is his user id, an integer assigned to each user on registration that never changes.

\subsubsection{GUI Look and Feel}

After using default GWT look, we decided to use Twitter Bootstrap -- good decision, looks nice and provides additional widgets.

Trying to be user friendly

- expected behavior

- as litle controls as possible

- can be controlled by mouse (except for typing)

- TranslationWorkspace can be controlled by keyboard (efficiency)

- other pages must be controlled by mouse ... user spends most time in Translation Workspace so occasional clicking should not be a problem; it feels more natural to us

changing data .. idea: it should be possible to change anything if there is not a strong reason why not

- change document title, movie title... by clicking on it (seems natural, added a tooltip)

- change timing or source text: we think that this is not usually required so it is under a double click to prevent unwanted activation (and a tooltip again)

- we offer the user the possibility to change the source text because if there is an error, the suggestions might be incorrect (the suggestions are regenerated on changing the source)

- we do not offer deleting chunks because this would be difficult to implement properly in GUI; however, the default behavior is not to export the untrabslated chunks, so not translating a chunk is roughly equivalent to deleting it

- we do not allow to change the source subtitle file (this would actually mean deleting the whole old document and creating a new one, probably only keeping the title -- so we leave this for the user to do himself); however, smaller edits can be done directly in TW

\subsubsection{Subtitle Formats}

first support for SRT and SUB

more and more problems with SUB

internal format of time is SUB

finally decided to support only SRT

- AFAIK 99\% of subtitles are in SRT

- easier to process (e.g. changing the timing), easier to sync to the player

We also support exporting the subtitles in plain text format, i.e. only the text without timing information.

\subsubsection{GUI Pages}

We started having one page only, the Translation Workspace, which was sufficient for development of the application for several months. Later we added the Document  Creator page as the default one, which replaced itself by the Translation Workspace once a document was created, and this was sufficient for some more time. However, we eventually could not avoid adding more pages and the need to explicitely handle multiple pages became obvious.

We immediately found out a week point of GWT here: it has nearly no native support for multiple pages. This was quite a surprising issue to us, since in any other web development technology or framework that we know, creating a new page and switchng pages are really simple tasks. GWT, however, observes the idea of having one HTML file and one Javascript file only that handle everything. (It is technically possible to create multiple pages by creating multiple GWT projects, each corresponding to one page, but this is not recommended for many good reasons.)

The GWT way of switching pages is by replacing the contents of the Root Panel (which represents the visible part of a page), or any other Panel placed within the Root Panel (which we decided to use so that some elements, such as the menu, are present on all pages). The switching is done without reloading, simply by creating an instance of the new page and directly setting it as the contents of the panel (without any convenience functions for that to our knowledge).

All the pages have the same URL by default, but GWT offers the History class for changing the URL and reacting to changes to URL (however, it is not linked to page switching by design -- this remains for each programmer to do by himself). There are also Hyperlink widgets, which represent a link to a different page, with the default action being changing the URL and invoking the appropriate handler (a ValueChangeHandler).
The URL contains an anchor with the name of the page (e.g. \#DocumentCreator or \#WelcomeScreen), and the History class is able to provide the name of the anchor to the user and to invoke the ValueChangeHandler when the URL changes.

We decided to use the History class and Hyperlinks to implement the menu and page switching. We create a PageHandler class which implements the ValueChangeHandler -- it turns the anchor string into a value of Page enum, checks whther the requested page can be loaded (especially based on the user log in / log out state) and loads the appropriate page. It also provides public methods to other parts of GUI so that they can switch pages, modifying the URL at the same time (so that the browser refresh works as expected).

However, we encountered one issue with the Hyperlinks: the ValueChangeHandler is invoked only if the value of URL changes, which means that if the user clicks a menu link which points to the page loaded (e.g. while creating a document, the user decides to restart and clicks the document creator link), the handler is not invoked. As this behavior is hard-coded into the Hyperlink class, we had to abandon the Hyperlinks and used NavLinks instead (a simple link with no default interaction with the History), with click handlers invoking the ValueChangeHandler.

When reviewing the resulting code, we found out that the History class and the ValueChangeHandler do not provide any benefit to us any more, so we removed them completely. Only then did we find out that the History class can also correctly handle the browser's forward/backward button behavior, so we readded it, but it is only used for that purpose now.

Another issue encountered was the fact that passing the page name as an anchor name is not always convenient and sometimes is not possible at all. If the page requires some additional parameters -- such as the password change page, where the user can change his password based on a link sent to his mailbox, containing a temporary token and the username as parameters -- the resulting address looks odd (e.g. http://filmtit.cz/?username=a\&token=df5ds3f\#ChangePassword). Moreover, the OpenID validation mechanism we used requires URL of a return page as a parameter, to which it adds parameters containing information about the validation process, but it is only able to add the parameters at the end of the address, resulting in an incorrect URL. For such cases, we decided to offer an alternative way of specifying the oage to be loaded in the URL by setting the page parameter (e.g. http://filmtit.cz/?page=ChangePassword\&username=a\&token=df5ds3f).

This solution finally proved to offer all features required.

We also had a discussion about the expected page to be loaded in various situations. E.g., if the user is in Translation Workspace and logs out, the URL page parameter stayed on Translation Workspace by default, although Welcome Screen is displayed instead because a logged out user does not have access to Translation Workspace. If he then logged back in without switching the page, the document he had opened before logging out was reopened again.
Eventually, the behavior was modified so that the URL is always changed to Welcome Page when the user explicitely logs out, but the URL is not modified if the user is logged out automatically, typically because his Session ID expires. Reasoning: if he was logged out implicitely, it is probable that he did not want that and woud like to continue with his work as soon as possible. However, if he logged out explicitely, he probably finished his work and expects the application to be ``closed'', i.e. not remembering the last stae of his work.

(TODO: tidy a little, I dont like that subsubsection very much...)

\subsubsection{Userspace Robustness}

Userspace started as nice but not robust -- it was not prepared to receiving erroneous data, it had concurrency issues especially when there were multiple users performing similar actions simultaneously, etc. This was probably caused by the fact that Userspace was developed several months before it was first used, due to delays in GUI development.

TODO: When we tried it out, we were getting tons of errors, so some checks were added and locks were introduced, so now it works nicely.

\subsubsection{Handling Parsing Errors}

At first, errors in parsing were not handled at all: the file was simply expected to be in the correct format, and if it was not, more or less random exceptions would be thrown from the parser.

We were improving the situation gradually, eventually creating a new exception class, InvalidDocumentFormatException, which is thrown by the parser, with a message containing details about the error that can be shown to the user.
Validation of times was improved by adding the SrtTime class (XXX odkaz, snad uz to nekde je), which checks even whether the number ranges are valid for the given field (e.g. whether the minute is between 0 and 59).

Also, as a preliminary check, we refuse any file which is larger than 500 kB. We introduced this check because it is very fast and efficient.

\subsubsection{Offline Mode}

At first, the whole app should be able to work offline.

Eventually, decided that the only part that must be able to work offline is the Translation Workspace.

Actually, only saving SetUserTranslation calls.

Decided to use Local Storage -- similar to cookies but better.

First idea: serialize and deserialize using JSON!
Later realization: would need an external library for that (TODO see RPCs), decided to write custom serialization.

How to identify the objects correctly:

- add a special object describing the other objets:

  - not reliable, user can interrupt the application at any time and can also change the values stored by the app

  - but could at least be used as a non-authoritative cache
  
  - now all object keys must be loaded to know the contents of the storage (but takes at most a few seconds in a typical case)

- each object contains all necessary information in its key

  - started with the ``key fields'' of the objects (document id and chunk index in case of SetUserTranslation)
  
  - then also added the class name to make it extendable (although it has not been extended)
  
  - then added username because there might be various FilmTit users using the same browser
  
  - and eventually changed username into user ID because now the user can change his username

\subsubsection{Hibernate Issues}

Before starting the project nobody of us had an experience with a object-relation mapping library for Java. Initially we were thinking also about not using such library and operate directly with the \emph{JDBC} connection. That was the option we chose for the translation memory core where we work only with few tables with not complicated relations. About the User Spaces where we expected much more complicated structure we thought that using a object-relation mapping would more beneficial and decided to use one. 

After doing a little research on the Internet we found out that Hibernate seems to be the most frequently used library for object relation mapping and decided to use it too. There were more than 20,000 discussions on \emph{stackoverflow.com} which gave us hope that solution for any problems we might have had could be easily found on the Internet. 

Details of the mapping itself could be found in section \ref{subsec:database_mapping}, the structure of the mapped database is in figure \ref{fig:em_of_us}. Here we will focus on the database mapping from the development perspective.

Because we started with Hibernate version 3, all properties we wanted make persistent must have had their getters and setters. The requirement to have a getter and setter for each property may sound as violating the encapsulation feature of the Object Oriented Programming, but luckily the method can be private. (Hibernate probably access them via reflection.) Classes also must have parameter-less constructor, but it also can be private.

New versions of Hibernate allow to use directly the fields of the classes, but since the mapped classes are usually wrappers of the shared classes, the fields are not directly accessible from the mapped class, we used the getters and setters for all properties to be consistent. Despite we ended up using Hibernate version 4.1.0, we are quite conservative in terms of using its features and keep all the mapping in separate XML files despite it can be done by annotations directly in the source file and do not access fields directly, but always use getters and setters.

Previously mentioned features of Hibernate were very often also source of bugs -- mostly forgetting to add a private setter. Very often mistake was also forgetting to add a property in the mapping and finding out it is not persistent. (When a new property was added it was usually not covered in tests at that time so it took some time find it.)

Another problem was that despite the Hibernate can adjust the database scheme to the mapping, it was often not able to change it in a proper way and it was necessary to drop the User Space tables in the database and let the Hibernate to create them from the scratch. This of course always caused loss of the data we tested the application on.

During debugging of one bug (it was happening that the same translation results was appearing in the database with different database IDs) it appeared how useful is to use the database constraints. If we had had it before this bug would have been found right away and it would not take so much time to find it. After this experience we added constrains everywhere where it was possible.

\subsubsection{Media Source}

was IMDB, is Freebase now

https problems

adding the "null" option

\subsubsection{GUI Refactoring}

TODO: have that here or not??? Yes!

Started as 1 class (GUI.java) which grew and grew.

The RPCs handling moved to a separate class.

Subgestbox class.

Introduced UiBinder for designing the GUI.

The notion of pages, each having its own java class and an UiBinder template.

Introducing the PageHandler.

The RPCs turned into individual classes in cz.filmtit.client.callables

The pages moved to cz.filmtit.client.pages and all are displayed simply by creating the class.

The notion of Dialogs, residing in cz.filmtit.client.dialogs, with Dialog being the superclass (peviously just hardcoded somewhere).

The sbgetsbox gets its own subpackage.

Gui made static

\subsubsection{Subgestboxes}

Input, RtfBox

Focus handling

Popping below/above/not at all

Scrolling to center

Resizing

Proper cross-browser CSS

...?

\subsubsection{Subtitles Export}

Necessary to give the user the possibility to export the subtitles.

Issue: GWT cannot create a file, neither on server (beacause it is all Javascript) nor on the user's computer (because of security restrictions).

Possible solutions:

- export the subtitles into a textarea

  - not really what a user expects

- send the exported subtitles to user's e-mail address

  - not really what a user expects
  
  - can be hard to use for the user
  
  - might be added as a special feature but should not be the only or primary way

- create a temporary file on the server and offer a download link to the user

  - creating temporary files is not very clean
  
  - should handle access rights in Jetty

- create a new servlet (using JSP) for downloading the file

  - we were afraid that it would be too much extra work
  
  - it turned out not to be so difficult so we used that

\subsubsection{Chunks Saving / Loading}

original idea: parse the subtitle file in GUI and keep it there; decision kept: no sense sending the chunks to US and then back again, also would cause unnecessary load on the server

problem: how to do editing when the Workspace expects to get the SRT file?

solution: Translation Workspace has two ways of accepting the data, one by getting the subtitle file contents and parsing it into chunks, the other one getting already parsed chunks from DB


problem: chunks were saved only upon requesting the translations for them, so part of document was lost if user closed too early :-)

solution: first save all chunks, then start loading the trabslation suggestions

\subsection{Final Development}
\label{subsec:final_development}

Optimizations required: better to throw away translation pairs and generated every time user logs in

Found many technical issues: %e.g. necessary to be able to stop loading generating the suggestions on the server at the moment - DONE


We left to the end a lot issues which we considered to be only minor technical , which appeared to be a lot of intensive work.

We estimated the 12$^\mathrm{th}$ August, 12 p.m. to be a day that we called the Feature Freeze which meant that after this date no new features were added to the project. At that time we started an intensive review of already existing code, debugging and working on the documentation. 

\section{Evaluating the Development Process}

One of the crucial decision for the project was the choice of technologies. Most of the technologies we used -- Maven, Scala, Hibernate, GWT -- were new to all of us and we also had not much experience with the other. Combining these technologies together was quite painful and would probably even for an experienced Java developer. Generally, we can say that the fact that we were not too much familiar with the technologies, we spent most of the time solving technical issues. There is quite a lot of research challenges, mostly in the fuzzy matching part, which had to remain untouched due to that.

On the other hand the choice of technologies appeared to be lucky that it probably saved a lot of work for us by the possibility to share implementation of class. Using the Scala language for the core also made the parallelization much easier.

Despite we spent quite a lot of time by the discussions how the structure of the project we didn't avoid a radical change of the design of the application 4 months after the project started. Nearly the whole User Space code had to be dropped and it was also necessary to totally remake the client components existing so far.

A bottleneck of the development process was also that not all of us were familiar with all of the technologies. It happened many times that somebody could not continue developing a particular part of the project and had to wait for an other team member to fix the issue even though it may have been just changing a single attribute in the Hibernate mapping.

Problems with communication, different communication platforms among time, members of the team sometimes must have waited for othres...



